{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rND5zX37DF6W"
      },
      "source": [
        "Import dataset '**imdb-reviews.csv**' from google drive and use pandas to parse.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DajPPSROD7rz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import pandas\n",
        "import re\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('omw-1.4')\n",
        "from textblob import Word \n",
        "from collections import Counter\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import learning_curve\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "id": "Ix4uiSfEjTux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6PV9OMUEzQ6"
      },
      "outputs": [],
      "source": [
        "dataset_path = '/content/drive/MyDrive/imdb-reviews.csv'\n",
        "testset_path = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pGUxVTfnFKrc"
      },
      "outputs": [],
      "source": [
        "df = pandas.read_csv(dataset_path, sep='\\t', engine='python')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHJ0cdOJHUkp"
      },
      "outputs": [],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4gjouuWHYR_"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KX8PiRmqICOx"
      },
      "source": [
        "Clear review comments. **Remove**:\n",
        "1.   *HTML*\n",
        "2.   *Numbers*\n",
        "3.   *Punctuation*\n",
        "4.   *Uppercase*\n",
        "5.   *Stopwords*\n",
        "6.   *Lemmatization*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymWN2mTVM3XN"
      },
      "outputs": [],
      "source": [
        "# HTML\n",
        "\n",
        "def remove_html(text):\n",
        "  return  re.sub('<.*?>', '', text)\n",
        "  \n",
        "def clean_html(df):\n",
        "  df['review'] = df['review'].apply(remove_html)\n",
        "  print(df.head())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcNFnkDLmOOw"
      },
      "outputs": [],
      "source": [
        "# Numbers\n",
        "\n",
        "def clean_numbers(df):\n",
        "  df['review'] = df['review'].str.replace(r'\\d+', '', regex=True)\n",
        "  print(df.head())\n",
        "  return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGkJm5tBnw4p"
      },
      "outputs": [],
      "source": [
        "# Punctuation\n",
        "\n",
        "def clean_punctuation(df):\n",
        "  df['review'] = df['review'].str.replace(r'[^\\w\\s]+', '', regex=True)\n",
        "  print(df.head())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwejltO4A7xY"
      },
      "outputs": [],
      "source": [
        "# Uppercase\n",
        "\n",
        "def clean_uppercase(df):\n",
        "  df['review'] = df['review'].str.lower()\n",
        "  print(df.head())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH8MMz6-Wnx1"
      },
      "source": [
        "Before we perform the other removals we need to tokenize the words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRqagsVXWm3U"
      },
      "outputs": [],
      "source": [
        "# Tokenization\n",
        "\n",
        "def clean_tokenize(df):\n",
        "  df['review'] = df['review'].apply(word_tokenize)\n",
        "  print(df.head())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "haPbgh7eDefm"
      },
      "outputs": [],
      "source": [
        "# Stopwords\n",
        "\n",
        "pattern = stopwords.words('english')\n",
        "\n",
        "def clean_stopwords(df):\n",
        "  df['review'] = df['review'].apply(lambda words: [w for w in words if w not in pattern])\n",
        "  print(df.head())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WvA2f1xK34K"
      },
      "outputs": [],
      "source": [
        "# Lemmatization\n",
        "\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# def lemmatize(text):\n",
        "#   return [lemmatizer.lemmatize(w) for w in word_tokenizer.tokenize(text)]\n",
        "\n",
        "def clean_lemmatize(df):\n",
        "  df['review'] = df['review'].apply(lambda word: [lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(lemmatizer.lemmatize(w, 'n'), 'a'), 'v'), 'r'), 's') for w in word])\n",
        "  print(df.head())\n",
        "  return df\n",
        "# df['review'] = df['review'].apply(lambda words: \" \".join([Word(x).lemmatize() for x in words]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bTeRK5qPM_4"
      },
      "source": [
        "Now we transform rating to 0 for negative (values in range [0, 4.0]) and 1 for positive (values in range [7.0, 10.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQ1tc2wXPfEG"
      },
      "outputs": [],
      "source": [
        "def clean_scale(df):\n",
        "  df['rating'] = df['rating'].apply(lambda x: 0 if x <= 4.0 else 1)\n",
        "  print(df.head())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ-UfYYoRL1r"
      },
      "source": [
        "Now we remove some rare words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ-16VgaRaq0"
      },
      "outputs": [],
      "source": [
        "# rarewords\n",
        "\n",
        "def clean_rarewords(df):\n",
        "  temp = df['review'].apply(lambda l: [item for item in l])\n",
        "  flat_list = [item for sublist in temp for item in sublist]\n",
        "  counter_list = Counter(flat_list).most_common()\n",
        "  final_list_desc = counter_list[:10]\n",
        "  final_list_asc  = counter_list[-10:]\n",
        "  only_first = [x for x,y in final_list_desc]\n",
        "  only_last  = [x for x,y in final_list_asc]\n",
        "\n",
        "  df['review'] = df['review'].apply(lambda words: [x for x in words if (x not in only_first) and (x not in only_last)])\n",
        "  print(df.head())\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9oQJj7FqLfi"
      },
      "outputs": [],
      "source": [
        "def transform(df):\n",
        "  df = clean_html(df)\n",
        "  df = clean_numbers(df)\n",
        "  df = clean_punctuation(df)\n",
        "  df = clean_uppercase(df)\n",
        "  df = clean_tokenize(df)\n",
        "  df = clean_stopwords(df)\n",
        "  df = clean_lemmatize(df)\n",
        "  df = clean_scale(df)\n",
        "  df = clean_rarewords(df)\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNPX7HQtq5va"
      },
      "outputs": [],
      "source": [
        "df = transform(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following experiment was added here after all the experiments below.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZQDA3pNNkCj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = 64\n",
        "\n",
        "while features <= 16500:\n",
        "  X = df['review'].apply(lambda x: ' '.join(x))\n",
        "  Y = df['rating']\n",
        "\n",
        "  tfidf = TfidfVectorizer(max_features=features)\n",
        "  X = tfidf.fit_transform(X)\n",
        "\n",
        "  train_sizes, train_scores, validation_scores = learning_curve(estimator = \n",
        "  LogisticRegression(solver='newton-cg', penalty='l2', C=3, max_iter=100), X = X, y = Y, train_sizes = np.linspace(0.0005, 0.999, 10), cv = 10)\n",
        "\n",
        "  print(\"Number of features: \", features)\n",
        "  train_scores_mean = train_scores.mean(axis=1)\n",
        "  validation_scores_mean = validation_scores.mean(axis=1)\n",
        "\n",
        "  plt.plot(train_sizes, train_scores_mean, label = 'Training score')\n",
        "  plt.plot(train_sizes, validation_scores_mean, label = 'Validation score')\n",
        "\n",
        "  plt.ylabel('Score')\n",
        "  plt.xlabel('Training size')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "  features *= 2\n"
      ],
      "metadata": {
        "id": "cgYd9CkFkK99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1sKAPnri1C6"
      },
      "outputs": [],
      "source": [
        "positive_reviews = df[df['rating'] == 0]\n",
        "negative_reviews = df[df['rating'] == 1]\n",
        "labels = ['Positive', 'Negative']\n",
        "\n",
        "sizes = [positive_reviews['rating'].count(), negative_reviews['rating'].count()]\n",
        "colors = [\"crimson\", \"lightsteelblue\"]\n",
        "\n",
        "explode = (0, 0.1)\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.3f%%',\n",
        "        shadow=True, startangle=45)\n",
        "ax1.axis('equal')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2KFsHSfTggr"
      },
      "source": [
        "Now we will start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hVRFnnAhniC"
      },
      "outputs": [],
      "source": [
        "# Logistic regression\n",
        "\n",
        "def do_Logistic_Regression(X_train, y_train, X_test, y_test, max_iterations):\n",
        "\n",
        "  LR = LogisticRegression(max_iter=max_iterations)\n",
        "  scores = cross_val_score(LR, X_train, y_train, cv=10, n_jobs=-1)\n",
        "  scores_mean = scores.mean()\n",
        "  print(scores_mean)\n",
        "\n",
        "  LR.fit(X_train, y_train)\n",
        "  y_test_predict = LR.predict(X_test)\n",
        "\n",
        "  print(classification_report(y_test, y_test_predict))\n",
        "\n",
        "  f1 = f1_score(y_test, y_test_predict)  \n",
        "  return f1, scores_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSPmJQUDTk8X"
      },
      "outputs": [],
      "source": [
        "# TFIDF\n",
        "\n",
        "def do_TFIDF(train_percent, features):\n",
        "  X = df['review'].apply(lambda x: ' '.join(x))\n",
        "  Y = df['rating']\n",
        "\n",
        "  tfidf = TfidfVectorizer(max_features=features)\n",
        "  X = tfidf.fit_transform(X)\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = train_percent, random_state = 13, stratify = Y)\n",
        "  \n",
        "  f1, scores_mean = do_Logistic_Regression(X_train, y_train, X_test, y_test, 400)\n",
        "\n",
        "  return f1, scores_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgU5opBGdNM7"
      },
      "outputs": [],
      "source": [
        "# Count Vectorizer\n",
        "\n",
        "def do_Count_Vectorizer(train_percent, features):\n",
        "  vectorizer = CountVectorizer(max_features=features, ngram_range=(1,3))\n",
        "\n",
        "  X = df['review'].apply(lambda x: ' '.join(x))\n",
        "  X = vectorizer.fit_transform(X)\n",
        "\n",
        "  Y = df['rating']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = train_percent, random_state = 13, stratify = Y)\n",
        "\n",
        "  f1, scores_mean = do_Logistic_Regression(X_train, y_train, X_test, y_test, 400)\n",
        "  return f1, scores_mean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o9XAuym1lGNp"
      },
      "outputs": [],
      "source": [
        "# Hashing Vectorizer\n",
        "\n",
        "def do_Hashing_Vectorizer(train_percent, features):\n",
        "  vectorizer = HashingVectorizer(n_features=features)\n",
        "  X = df['review'].apply(lambda x: ' '.join(x))\n",
        "  X = vectorizer.fit_transform(X)\n",
        "\n",
        "  Y = df['rating']\n",
        "\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = train_percent, random_state = 13, stratify = Y)\n",
        "\n",
        "  f1, scores_mean = do_Logistic_Regression(X_train, y_train, X_test, y_test, 400)\n",
        "  return f1, scores_mean"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJc3Hv6osba5"
      },
      "source": [
        "We will test these three algorithms with different train sizes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w--vaBluslA_"
      },
      "outputs": [],
      "source": [
        "# Testing TFIDF\n",
        "train_percent = 0.2\n",
        "\n",
        "xs = []\n",
        "scores = []\n",
        "f1s = []\n",
        "\n",
        "while train_percent <= 0.99:\n",
        "  f1, mean_score = do_TFIDF(train_percent, 40000)\n",
        "  \n",
        "  xs.append(train_percent)\n",
        "  scores.append(mean_score)\n",
        "  f1s.append(f1)\n",
        "\n",
        "  train_percent += 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vz4FAlUawgp7"
      },
      "outputs": [],
      "source": [
        "print(xs)\n",
        "print(scores)\n",
        "plt.plot(xs, scores, marker=\"x\")\n",
        "plt.plot(xs, f1s, marker=\"*\")\n",
        "plt.legend([\"Mean score\", \"F1 score\"])\n",
        "plt.xlabel(\"train percent\")\n",
        "plt.ylabel(\"score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2qXrZVK32V7Z"
      },
      "outputs": [],
      "source": [
        "# Testing Count Vectorizer\n",
        "\n",
        "train_percent = 0.2\n",
        "\n",
        "xs = []\n",
        "scores = []\n",
        "f1s = []\n",
        "\n",
        "while train_percent <= 0.99:\n",
        "  f1, mean_score = do_Count_Vectorizer(train_percent, 40000)\n",
        "  \n",
        "  xs.append(train_percent)\n",
        "  scores.append(mean_score)\n",
        "  f1s.append(f1)\n",
        "  \n",
        "  train_percent += 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZqjJ42Tu2eah"
      },
      "outputs": [],
      "source": [
        "print(xs)\n",
        "print(scores)\n",
        "plt.plot(xs, scores, marker=\"x\")\n",
        "plt.plot(xs, f1s, marker=\"*\")\n",
        "plt.legend([\"Mean score\", \"F1 score\"])\n",
        "plt.xlabel(\"train percent\")\n",
        "plt.ylabel(\"score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZldDDpDG4k1n"
      },
      "outputs": [],
      "source": [
        "# Testing Hashing Vectorizer\n",
        "\n",
        "train_percent = 0.2\n",
        "\n",
        "xs = []\n",
        "scores = []\n",
        "f1s = []\n",
        "\n",
        "while train_percent <= 0.99:\n",
        "  f1, mean_score = do_Hashing_Vectorizer(train_percent, 40000)\n",
        "  \n",
        "  xs.append(train_percent)\n",
        "  scores.append(mean_score)\n",
        "  f1s.append(f1)\n",
        "  \n",
        "  train_percent += 0.15"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KQVQizeSUkV7"
      },
      "outputs": [],
      "source": [
        "print(xs)\n",
        "print(scores)\n",
        "plt.plot(xs, scores, marker=\"x\")\n",
        "plt.plot(xs, f1s, marker=\"*\")\n",
        "plt.legend([\"Mean score\", \"F1 score\"])\n",
        "plt.xlabel(\"train percent\")\n",
        "plt.ylabel(\"score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qwK8p1n6834"
      },
      "source": [
        "We will test the algorithms with different max features values.\n",
        "We will use powers of 2 for possible values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HID2Y7Jl7BJd"
      },
      "outputs": [],
      "source": [
        "# Testing TFIDF\n",
        "\n",
        "\n",
        "features=1024\n",
        "\n",
        "xs = []\n",
        "scores = []\n",
        "f1s = []\n",
        "\n",
        "while features <= 40000:\n",
        "  f1, mean_score = do_TFIDF(0.80, features)\n",
        "  \n",
        "  xs.append(features)\n",
        "  scores.append(mean_score)\n",
        "  f1s.append(f1)\n",
        "  \n",
        "  features *= 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWCcku2I-VdY"
      },
      "outputs": [],
      "source": [
        "print(xs)\n",
        "print(scores)\n",
        "plt.plot(xs, scores, marker=\"x\")\n",
        "plt.plot(xs, f1s, marker=\"*\")\n",
        "plt.legend([\"Mean score\", \"F1 score\"])\n",
        "plt.xlabel(\"features\")\n",
        "plt.ylabel(\"score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NBsvqfB_mrp"
      },
      "outputs": [],
      "source": [
        "# Testing Count Vectorizer\n",
        "\n",
        "features=1024\n",
        "\n",
        "xs = []\n",
        "scores = []\n",
        "f1s = []\n",
        "\n",
        "while features <= 40000:\n",
        "  f1, mean_score = do_Count_Vectorizer(0.80, features)\n",
        "  \n",
        "  xs.append(features)\n",
        "  scores.append(mean_score)\n",
        "  f1s.append(f1)\n",
        "  \n",
        "  features *= 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE31zUwl_3UJ"
      },
      "outputs": [],
      "source": [
        "print(xs)\n",
        "print(scores)\n",
        "plt.plot(xs, scores, marker=\"x\")\n",
        "plt.plot(xs, f1s, marker=\"*\")\n",
        "plt.legend([\"Mean score\", \"F1 score\"])\n",
        "plt.xlabel(\"features\")\n",
        "plt.ylabel(\"score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntfzYXyE_47M"
      },
      "outputs": [],
      "source": [
        "# Testing Hashing Vectorizer\n",
        "\n",
        "features=1024\n",
        "\n",
        "xs = []\n",
        "scores = []\n",
        "f1s = []\n",
        "\n",
        "while features <= 40000:\n",
        "  f1, mean_score = do_Hashing_Vectorizer(0.80, features)\n",
        "  \n",
        "  xs.append(features)\n",
        "  scores.append(mean_score)\n",
        "  f1s.append(f1)\n",
        "  \n",
        "  features *= 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LajjS_D3Ba3r"
      },
      "outputs": [],
      "source": [
        "print(xs)\n",
        "print(scores)\n",
        "plt.plot(xs, scores, marker=\"x\")\n",
        "plt.plot(xs, f1s, marker=\"*\")\n",
        "plt.legend([\"Mean score\", \"F1 score\"])\n",
        "plt.xlabel(\"features\")\n",
        "plt.ylabel(\"score\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bH7bfaVM_TCl"
      },
      "outputs": [],
      "source": [
        "model = LogisticRegression()\n",
        "solvers = ['newton-cg', 'lbfgs', 'liblinear']\n",
        "penalty = ['l1', 'l2']\n",
        "c_values = [7, 5, 3, 0.1, 0.01]\n",
        "m_iter = [100, 1000]\n",
        "grid = dict(solver=solvers,penalty=penalty,C=c_values, max_iter=m_iter)\n",
        "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=10, scoring='f1', error_score=0)\n",
        "\n",
        "X = df['review'].apply(lambda x: ' '.join(x))\n",
        "Y = df['rating']\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=16500)\n",
        "X = tfidf.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size = 0.80, random_state = 13, stratify = Y)\n",
        "\n",
        "grid_result = grid_search.fit(X_train, y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bjEwv42OcAlj"
      },
      "outputs": [],
      "source": [
        "print(\"Best result: %f using parameters: %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPVNtBf75sSU"
      },
      "source": [
        "From now on we will use train_percent = 0.80 (Pareto principle).\n",
        "\n",
        "For our vectorizer we will use TFIDF (proved to be better than the three different methods).\n",
        "\n",
        "For TFIDF we will use max_features = 16500 (close to best).\n",
        "\n",
        "We will use LogisticRegression with parameters: {'C': 3, 'max_iter': 100, 'penalty': 'l2', 'solver': 'newton-cg'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWfp6ypD8lv7"
      },
      "outputs": [],
      "source": [
        "# Confusion matrix\n",
        "\n",
        "X = df['review'].apply(lambda x: ' '.join(x))\n",
        "Y = df['rating']\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=16500)\n",
        "X = tfidf.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.80, random_state=13, stratify=Y)\n",
        "\n",
        "LR = LogisticRegression(solver='newton-cg', penalty='l2', C=3, max_iter=100)\n",
        "LR.fit(X_train, y_train)\n",
        "y_test_predict = LR.predict(X_test)\n",
        "\n",
        "confusion_matrix = confusion_matrix(y_test, y_test_predict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL-IDZoyGTDk"
      },
      "outputs": [],
      "source": [
        "print(confusion_matrix)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(confusion_matrix)\n",
        "fig.colorbar(cax)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iUkF9rOHns3"
      },
      "source": [
        "*   True positive: 3977\n",
        "*   False positive: 523\n",
        "*   True negative: 4051\n",
        "*   False negative: 451\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H64Ad-hIPinh"
      },
      "outputs": [],
      "source": [
        "train_sizes, train_scores, validation_scores = learning_curve(estimator = \n",
        "LogisticRegression(solver='newton-cg', penalty='l2', C=3, max_iter=100), X = X, y = Y, train_sizes = np.linspace(0.0005, 0.999, 20), cv = 10)\n",
        "\n",
        "print('Training scores:\\n\\n', train_scores)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyomjgauRuyw"
      },
      "outputs": [],
      "source": [
        "train_scores_mean = train_scores.mean(axis=1)\n",
        "validation_scores_mean = validation_scores.mean(axis=1)\n",
        "\n",
        "plt.plot(train_sizes, train_scores_mean, label = 'Training score')\n",
        "plt.plot(train_sizes, validation_scores_mean, label = 'Validation score')\n",
        "\n",
        "plt.ylabel('Score')\n",
        "plt.xlabel('Training size')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XY7g8fwpBQU"
      },
      "outputs": [],
      "source": [
        "if testset_path != None:\n",
        "  # Training\n",
        "\n",
        "  df = pandas.read_csv(dataset_path, sep='\\t', engine='python')\n",
        "  df = transform(df)\n",
        "  X = df['review'].apply(lambda x: ' '.join(x))\n",
        "  Y = df['rating']\n",
        "\n",
        "  tfidf = TfidfVectorizer(max_features=16500)\n",
        "  X = tfidf.fit_transform(X)\n",
        "\n",
        "  LR = LogisticRegression(solver='newton-cg', penalty='l2', C=3, max_iter=100)\n",
        "  LR.fit(X, Y)\n",
        "\n",
        "  # Testing\n",
        "  df_test = pandas.read_csv(testset_path, sep='\\t', engine='python')\n",
        "  df_test = transform(df_test)\n",
        "  X_test = df_test['review'].apply(lambda x: ' '.join(x))\n",
        "  Y_test = df_test['rating']\n",
        "\n",
        "  tfidf = TfidfVectorizer(max_features=16500)\n",
        "  X_test = tfidf.fit_transform(X_test)\n",
        "\n",
        "  test_predict = LR.predict(X_test)\n",
        "  print(classification_report(Y_test, test_predict))\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}